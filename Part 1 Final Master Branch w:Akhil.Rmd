---
title: "Exercise 2"
author: "Alex Rados,  Sri Jonnalagadda, Kenny Kato"
output: 
  github_document:
    toc: True
---
# Saratoga Housing Prices
```{r include=FALSE}
library(tidyverse)
library(mosaic)
library(foreach)
library(doMC)
options('mosaic:parallelMessage' = FALSE)
data(SaratogaHouses)
``` 

## Overview  
The goal at hand is determining a good model for predicting future house prices in Saratoga, New York in order to know how much to tax property owners for the local taxing authority.

## Data  
We will be looking at past Saratoga housing prices to gather reasonable expectations of the effects that house characteristics will have on future price. Some of the variables at hand are age of the house (in years), percentage of residents in the neighborhood with a college degree, number of bedrooms and bathrooms, among many others. 

Using these variables to fit both linear and non-parametric models, we are trying to fit the most accurate model when predicting house prices. This can be measured in the form of the root mean-squared error (RMSE) of that specific model. 

```{r include=FALSE}
# Building a RMSE function
rmse = function(y, yhat) {
  sqrt( mean( (y - yhat)^2 ) )
}
``` 

In order to generate the most accurate predictions, however, we can't judge it based on past data that we have. Thus, we randomly sampled 80% of the data at hand and used that as a training set on which we built our model while using the other 20% as a test set on which we tested our model to gather a sufficient RMSE. This then allows us to measure the out-of-sample performance.

```{r include=FALSE}
# Split into training and testing sets
n = nrow(SaratogaHouses)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
``` 

Finally, to address the issue of random variation with the selection of data points that end up in our train/test split, we ran a do-loop. This averaged the estimate of out-of-sample RMSEs over 500 different random train/test splits, to which we felt comfortable with the lack of random variation in the results that were being generated.

## Linear Model
I initially began with a linear model that predicted price using age, percentage of college graduates in the neighborhood, size of the lot (in acres), number of bedrooms, bathrooms, and total rooms, living area of the house (in square feet), number of fireplaces, type of heating system, type of fuel system, and whether there was central air conditioning. This produced a lower RMSE, around 66,000, than either a smaller model (just lot size, bedrooms, and bathrooms) or a larger model (all these variables and every interaction between them). 

```{r echo=FALSE, warning=FALSE, cache=TRUE}
# Creating a bunch of different train/test splits
# Gathering predictions out of sample
# Gathering the RMSE of those predictions and getting the means of all 500
out_lm_medium = do(500)*{
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  saratoga_train = SaratogaHouses[train_cases,]
  saratoga_test = SaratogaHouses[test_cases,]
  lm_medium = lm(price ~ . - (sewer + waterfront + landValue + newConstruction), data=saratoga_train)
  
  yhat_test_med = predict(lm_medium, saratoga_test)
  rmse(saratoga_test$price, yhat_test_med)
} 
mean(out_lm_medium$result)
``` 

Going off this model, we attempted to improve the RMSE through selections of specific interaction and adding/subtracting variables. After going through a muiltitude of subtractions, additions, and interactions, we finally settled on a model that improved the RMSE from around 66,000 to about 64,000. 

First, we made an adjustment to the data in creating an extra rooms variable that was the total amount of rooms minus the bedrooms and bathrooms, as before in the regression the variable total rooms would've included two other features already in the model. Thus, we added in extra rooms and took out the total rooms. 

```{r include=FALSE}
SaratogaHouses = mutate(SaratogaHouses, extraRooms = rooms - bedrooms - bathrooms)
``` 

We then added interactions between bedrooms and bathrooms, bathrooms and extra rooms, and bedrooms and extra rooms. We also included whether there was new construction on the house, along with whether the property is on the water front. Adding in new construction and the interactions improved the RMSE by 300. The most impressive additional variable beyond the initial model was waterfront, which dropped the RMSE by about 2,000. This is the linear model that we decided on.

```{r echo=FALSE, warning=FALSE, cache=TRUE}
out_lm6 = do(500)*{
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  saratoga_train = SaratogaHouses[train_cases,]
  saratoga_test = SaratogaHouses[test_cases,]
  lm6 = lm(price ~ lotSize + bedrooms + bathrooms + bedrooms*extraRooms +
             extraRooms + bedrooms*bathrooms + bathrooms*extraRooms + 
             age + livingArea + pctCollege + fireplaces + heating + 
             fuel + waterfront + newConstruction + centralAir, data=saratoga_train)
  
  yhat_test6 = predict(lm6, saratoga_test)
  rmse(saratoga_test$price, yhat_test6)
} 
mean(out_lm6$result)
``` 

## Non-Parametric Model
Next, we wanted to try to improve upon the linear fit by building a non-parametric K nearest neighbors model that uses those same variables to derive better performance. We ran a do-loop around the KNN assignments in order to get rid of the random variation in terms of choosing the train/test splits. 

```{r include=FALSE, warning=FALSE}
###### *****KNN Part***** #####
library(class)
library(FNN)
```

```{r echo=FALSE, warning=FALSE, cache=TRUE}
k_grid = exp(seq(log(1), log(300), length=100)) %>% round %>% unique
rmse_grid = foreach(K = k_grid, .combine='c') %do% {
  out_knn = do(500)*{
    train_cases = sample.int(n, n_train, replace=FALSE)
    test_cases = setdiff(1:n, train_cases)
    saratoga_train = SaratogaHouses[train_cases,]
    saratoga_test = SaratogaHouses[test_cases,]
    
    Xtrain = model.matrix(~ . - (price + sewer + landValue + rooms) - 1, data=saratoga_train)
    Xtest = model.matrix(~ . - (price + sewer + landValue + rooms) - 1, data=saratoga_test)
    ytrain = saratoga_train$price 
    ytest = saratoga_test$price
    scale_train = apply(Xtrain, 2, sd) 
    Xtilde_train = scale(Xtrain, scale = scale_train)
    Xtilde_test = scale(Xtest, scale = scale_train) 
    
    # Fit KNN models
    knn_model = knn.reg(Xtilde_train, Xtilde_test, ytrain, k=K)
    rmse(ytest, knn_model$pred) 
  }
  mean(out_knn$result)
}

plot(k_grid, rmse_grid, log='x', ylim = c(60000,75000), xlab = "K", ylab = "RMSE")
abline(h=mean(out_lm6$result), col = "red")

mean(out_knn$result)
mean(out_lm6$result)
``` 

## Conclusion
We found that a non-parametric model was not able to improve upon our linear fit. The KNN method of modeling resulted in a consistently higher RMSE, thus showing the use of a linear model compared to a non-parametric fit in predicting house prices for taxation purposes.

Therefore, it would be my recommendation that the local taxing authority uses the linear model that encompasses age, percentage of college graduates in the neighborhood, size of the lot (in acres), number of bedrooms, bathrooms, and extra rooms, living area of the house (in square feet), number of fireplaces, type of heating system, type of fuel system, whether there was central air conditioning, along with interactions between bedrooms and bathrooms, bathrooms and extra rooms, bedrooms and extra rooms, and, finally, if there was new construction and if the property was on the waterfront. This model produced the lowest out-of-sample root mean-squared error, giving it the most accurate option for predicting future house price for generating an accurate tax for. 

# A Hospital Audit
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{R include=FALSE}
library(dplyr)
library(openxlsx)
library(ggplot2)
library(tidyverse)
library(survival)
library(ggfortify)
library(data.table)
library(openxlsx)
library(knitr)
## load data and remove unneeded variables 
brca <- read_csv('~/Documents/Data Mining/data/brca.csv')
####First question: are some radiologists more clinically conservative than others in recalling patients, holding patient risk factors equal?
temp = data.frame(brca)
unique(brca$radiologist)
R.13 = subset(temp, radiologist == "radiologist13")
R.34 = subset(temp, radiologist == "radiologist34")
R.66 = subset(temp, radiologist == "radiologist66")
R.89 = subset(temp, radiologist == "radiologist89")
R.95 = subset(temp, radiologist == "radiologist95")
reg13 = lm(recall ~ history  ,symptoms , data=R.13)
reg34 = lm(recall ~ history , data=R.34)
reg66 = lm(recall ~ history , data=R.66)
reg89 = lm(recall ~ history  , data=R.89)
reg95 = lm(recall ~ history, data=R.95)
#### Radiologist 13##
logit_13= glm(recall ~ history+symptoms+age, data=R.13, family='binomial')
phat_test_13 = predict(logit_13, R.13, type='response') 
 yhat_test_13= ifelse(phat_test_13> 0.5, 1, 0) 
confusion_13 = table(y = R.13$recall, yhat = yhat_test_13) 
confusion_13
table(R.13$recall)
29/sum(table(R.13$recall))
#### Radiologist 34####
logit_34 = glm(recall ~ age+history+symptoms, data=R.34, family='binomial')
phat_test_34 = predict(logit_34, R.34, type='response') 
yhat_test_34 = ifelse(phat_test_34 > 0.5, 1, 0) 
confusion_34 = table(y = R.34$recall, yhat = yhat_test_34) 
confusion_34
table(R.34$recall)
17/sum(table(R.34$recall))
##### Radiologist 66 ###
logit_66 = glm(recall ~ age+history+symptoms, data=R.66, family='binomial')
phat_test_66 = predict(logit_66, R.66, type='response') 
yhat_test_66 = ifelse(phat_test_66> 0.5, 1, 0) 
confusion_out_66 = table(y = R.66$recall, yhat = yhat_test_66)
confusion_out_66
table(R.66$recall)
37/sum(table(R.34$recall))
#### radiologist 89 ###
logit_89 = glm(recall ~ age+history+symptoms, data=R.89, family='binomial')
phat_test_89 = predict(logit_89, R.89, type='response') 
yhat_test_89 = ifelse(phat_test_89 > 0.5, 1, 0) 
confusion_89= table(y = R.89$recall, yhat = yhat_test_89) 
confusion_89
table(R.89$recall)
38/sum(table(R.89$recall))
##3# radioligtst 95 ##
logit_95 = glm(recall ~ age+history+symptoms, data=R.95, family='binomial')
phat_test_95 = predict(logit_95, R.95, type='response') 
yhat_test_95 = ifelse(phat_test_95 > 0.5, 1, 0) 
confusion_95 = table(y = R.95$recall, yhat = yhat_test_95) 
confusion_95
table(R.95$recall)
27/sum(table(R.95$recall))
recall.rate= c(.146, .086, .1878, .193, .137)
radiologist= c( "13", "34","66","89","95")
df <- melt(data.frame(radiologist,recall.rate))
keeps <- c("radiologist", "value")
`````

## Part 1
We had the data from five radiologist who saw about 197 patients each. When looking at recall rates for each radiologist we split the patients accordingly. We then created a linear probability model which accounts for the risk factors such as history, symptoms, and age when predicting each recall rate. 

We find that some of these medical professionals are more prone to recalls than others. The middle of our group ended up at a 14.65% (radiologist 13) call back. Radiologist 89 had the highest recall rate at 19.30% and radiologist 34 had the lowest recall rate of 8.63%. This shows there is a range of recall rates dependent on the person seeing the charts. We see our more conservative radiologists, such as 89, and our more lenient radiologist, such as 34. This must be taken into consideration when seeing a radiologist, as the level of conservatism could have a major effect. For reference radiologist 66 had a 18.78% recall rate and 95 had 13.70% recall rate.

 ````{R include=FALSE}
###model A###
logit= glm(cancer ~ recall, data=brca, family='binomial')
phat_test= predict(logit, brca, type='response') 
yhat = ifelse(phat_test> 0.5, 1, 0) 
confusion = table(y = brca$cancer, yhat = yhat) 
confusion
table(brca$cancer)
1/sum(table(brca$recall))
### model B ###
logitB= glm(cancer ~ recall+history+age+symptoms+menopause, data=brca, family='binomial')
coefficients(logitB)
coef(logit) 
phat_testB= predict(logitB, brca, type='response') 
yhatB = ifelse(phat_testB> 0.5, 1, 0) 
````

## PART 2
The purpose of our two models are to see the effects of taking into account factors relating to the patient in whether or not cancer is caught in the first 12 months. 
   
In model A we regressed recall on cancer findings. What we found is that if a patient is recalled they are about 10 times as likely to have cancer. This logic is valid as the more in depth the screening the higher likelihood of cancer being found.
  
In model B we regress with other risk factors to see how they affect the chances of cancer and how the recall rate is effected. What we see is that there is a similar increase in probability of finding the cancer if there is a callback. The remaining risk factors also show an increase in cancer probability when factored in. The exception to this was those patients post-menopause and no hormonal treatment who faced a .787 scale likelihood of cancer being found. Another highlight is that the patients older than 70 are about 3 times more likely for cancer to be discovered. This was the highest risk factor other than being recalled. 

## Conclusion
What we have learned is that when addressing these patients age should be the most heavily weighted in our diagnosis/recall request. The factor of being post-menopause and no hormone therapy should weight the radiologist to not recall the patient for the possibility of having cancer.