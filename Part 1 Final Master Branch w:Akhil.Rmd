---
title: "Exercise 2"
author: "Alex Rados, Sri Jonnalagadda, Kenny Kato"
output: 
  github_document:
    toc: True
---
# Saratoga Housing Prices
```{r include=FALSE}
library(tidyverse)
library(mosaic)
library(foreach)
library(doMC)
options('mosaic:parallelMessage' = FALSE)
data(SaratogaHouses)
``` 

## Overview  
The goal at hand is determining a good model for predicting future house prices in Saratoga, New York in order to know how much to tax property owners for the local taxing authority.

## Data  
We will be looking at past Saratoga housing prices to gather reasonable expectations of the effects that house characteristics will have on future price. Some of the variables at hand are age of the house (in years), percentage of residents in the neighborhood with a college degree, number of bedrooms and bathrooms, among many others. 

Using these variables to fit both linear and non-parametric models, we are trying to fit the most accurate model when predicting house prices. This can be measured in the form of the root mean-squared error (RMSE) of that specific model. 

```{r include=FALSE}
# Building a RMSE function
rmse = function(y, yhat) {
  sqrt( mean( (y - yhat)^2 ) )
}
``` 

In order to generate the most accurate predictions, however, we can't judge it based on past data that we have. Thus, we randomly sampled 80% of the data at hand and used that as a training set on which we built our model while using the other 20% as a test set on which we tested our model to gather a sufficient RMSE. This then allows us to measure the out-of-sample performance.

```{r include=FALSE}
# Split into training and testing sets
n = nrow(SaratogaHouses)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
``` 

Finally, to address the issue of random variation with the selection of data points that end up in our train/test split, we ran a do-loop. This averaged the estimate of out-of-sample RMSEs over 500 different random train/test splits, to which we felt comfortable with the lack of random variation in the results that were being generated.

## Linear Model
I initially began with a linear model that predicted price using age, percentage of college graduates in the neighborhood, size of the lot (in acres), number of bedrooms, bathrooms, and total rooms, living area of the house (in square feet), number of fireplaces, type of heating system, type of fuel system, and whether there was central air conditioning. This produced a lower RMSE, around 66,000, than either a smaller model (just lot size, bedrooms, and bathrooms) or a larger model (all these variables and every interaction between them). 

```{r echo=FALSE, warning=FALSE, cache=TRUE}
# Creating a bunch of different train/test splits
# Gathering predictions out of sample
# Gathering the RMSE of those predictions and getting the means of all 500
out_lm_medium = do(500)*{
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  saratoga_train = SaratogaHouses[train_cases,]
  saratoga_test = SaratogaHouses[test_cases,]
  lm_medium = lm(price ~ . - (sewer + waterfront + landValue + newConstruction), data=saratoga_train)
  
  yhat_test_med = predict(lm_medium, saratoga_test)
  rmse(saratoga_test$price, yhat_test_med)
} 
mean(out_lm_medium$result)
``` 

Going off this model, we attempted to improve the RMSE through selections of specific interaction and adding/subtracting variables. After going through a muiltitude of subtractions, additions, and interactions, we finally settled on a model that improved the RMSE from around 66,000 to about 64,000. 

First, we made an adjustment to the data in creating an extra rooms variable that was the total amount of rooms minus the bedrooms and bathrooms, as before in the regression the variable total rooms would've included two other features already in the model. Thus, we added in extra rooms and took out the total rooms. 

```{r include=FALSE}
SaratogaHouses = mutate(SaratogaHouses, extraRooms = rooms - bedrooms - bathrooms)
``` 

We then added interactions between bedrooms and bathrooms, bathrooms and extra rooms, and bedrooms and extra rooms. We also included whether there was new construction on the house, along with whether the property is on the water front. Adding in new construction and the interactions improved the RMSE by 300. The most impressive additional variable beyond the initial model was waterfront, which dropped the RMSE by about 2,000. This is the linear model that we decided on.

```{r echo=FALSE, warning=FALSE, cache=TRUE}
out_lm6 = do(500)*{
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  saratoga_train = SaratogaHouses[train_cases,]
  saratoga_test = SaratogaHouses[test_cases,]
  lm6 = lm(price ~ lotSize + bedrooms + bathrooms + bedrooms*extraRooms +
             extraRooms + bedrooms*bathrooms + bathrooms*extraRooms + 
             age + livingArea + pctCollege + fireplaces + heating + 
             fuel + waterfront + newConstruction + centralAir, data=saratoga_train)
  
  yhat_test6 = predict(lm6, saratoga_test)
  rmse(saratoga_test$price, yhat_test6)
} 
mean(out_lm6$result)
``` 

## Non-Parametric Model
Next, we wanted to try to improve upon the linear fit by building a non-parametric K nearest neighbors model that uses those same variables to derive better performance. We ran a do-loop around the KNN assignments in order to get rid of the random variation in terms of choosing the train/test splits. 

```{r include=FALSE, warning=FALSE}
###### *****KNN Part***** #####
library(class)
library(FNN)
```

```{r echo=FALSE, warning=FALSE, cache=TRUE}
k_grid = exp(seq(log(1), log(300), length=100)) %>% round %>% unique
rmse_grid = foreach(K = k_grid, .combine='c') %do% {
  out_knn = do(500)*{
    train_cases = sample.int(n, n_train, replace=FALSE)
    test_cases = setdiff(1:n, train_cases)
    saratoga_train = SaratogaHouses[train_cases,]
    saratoga_test = SaratogaHouses[test_cases,]
    
    Xtrain = model.matrix(~ . - (price + sewer + landValue + rooms) - 1, data=saratoga_train)
    Xtest = model.matrix(~ . - (price + sewer + landValue + rooms) - 1, data=saratoga_test)
    ytrain = saratoga_train$price 
    ytest = saratoga_test$price
    scale_train = apply(Xtrain, 2, sd) 
    Xtilde_train = scale(Xtrain, scale = scale_train)
    Xtilde_test = scale(Xtest, scale = scale_train) 
    
    # Fit KNN models
    knn_model = knn.reg(Xtilde_train, Xtilde_test, ytrain, k=K)
    rmse(ytest, knn_model$pred) 
  }
  mean(out_knn$result)
}

plot(k_grid, rmse_grid, log='x', ylim = c(60000,75000), xlab = "K", ylab = "RMSE")
abline(h=mean(out_lm6$result), col = "red")

mean(out_knn$result)
mean(out_lm6$result)
``` 

## Conclusion
We found that a non-parametric model was not able to improve upon our linear fit. The KNN method of modeling resulted in a consistently higher RMSE, thus showing the use of a linear model compared to a non-parametric fit in predicting house prices for taxation purposes.

Therefore, it would be my recommendation that the local taxing authority uses the linear model that encompasses age, percentage of college graduates in the neighborhood, size of the lot (in acres), number of bedrooms, bathrooms, and extra rooms, living area of the house (in square feet), number of fireplaces, type of heating system, type of fuel system, whether there was central air conditioning, along with interactions between bedrooms and bathrooms, bathrooms and extra rooms, bedrooms and extra rooms, and, finally, if there was new construction and if the property was on the waterfront. This model produced the lowest out-of-sample root mean-squared error, giving it the most accurate option for predicting future house price for generating an accurate tax for. 

# A Hospital Audit
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{R include=FALSE}
library(dplyr)
library(openxlsx)
library(ggplot2)
library(tidyverse)
library(survival)
library(ggfortify)
library(data.table)
library(openxlsx)
library(knitr)
## load data and remove unneeded variables 
brca <- read_csv('~/Documents/Data Mining/data/brca.csv')
####First question: are some radiologists more clinically conservative than others in recalling patients, holding patient risk factors equal?
temp = data.frame(brca)
unique(brca$radiologist)
R.13 = subset(temp, radiologist == "radiologist13")
R.34 = subset(temp, radiologist == "radiologist34")
R.66 = subset(temp, radiologist == "radiologist66")
R.89 = subset(temp, radiologist == "radiologist89")
R.95 = subset(temp, radiologist == "radiologist95")
reg13 = lm(recall ~ history  ,symptoms , data=R.13)
reg34 = lm(recall ~ history , data=R.34)
reg66 = lm(recall ~ history , data=R.66)
reg89 = lm(recall ~ history  , data=R.89)
reg95 = lm(recall ~ history, data=R.95)
#### Radiologist 13##
logit_13= glm(recall ~ history+symptoms+age, data=R.13, family='binomial')
phat_test_13 = predict(logit_13, R.13, type='response') 
 yhat_test_13= ifelse(phat_test_13> 0.5, 1, 0) 
confusion_13 = table(y = R.13$recall, yhat = yhat_test_13) 
confusion_13
table(R.13$recall)
29/sum(table(R.13$recall))
#### Radiologist 34####
logit_34 = glm(recall ~ age+history+symptoms, data=R.34, family='binomial')
phat_test_34 = predict(logit_34, R.34, type='response') 
yhat_test_34 = ifelse(phat_test_34 > 0.5, 1, 0) 
confusion_34 = table(y = R.34$recall, yhat = yhat_test_34) 
confusion_34
table(R.34$recall)
17/sum(table(R.34$recall))
##### Radiologist 66 ###
logit_66 = glm(recall ~ age+history+symptoms, data=R.66, family='binomial')
phat_test_66 = predict(logit_66, R.66, type='response') 
yhat_test_66 = ifelse(phat_test_66> 0.5, 1, 0) 
confusion_out_66 = table(y = R.66$recall, yhat = yhat_test_66)
confusion_out_66
table(R.66$recall)
37/sum(table(R.34$recall))
#### radiologist 89 ###
logit_89 = glm(recall ~ age+history+symptoms, data=R.89, family='binomial')
phat_test_89 = predict(logit_89, R.89, type='response') 
yhat_test_89 = ifelse(phat_test_89 > 0.5, 1, 0) 
confusion_89= table(y = R.89$recall, yhat = yhat_test_89) 
confusion_89
table(R.89$recall)
38/sum(table(R.89$recall))
##3# radioligtst 95 ##
logit_95 = glm(recall ~ age+history+symptoms, data=R.95, family='binomial')
phat_test_95 = predict(logit_95, R.95, type='response') 
yhat_test_95 = ifelse(phat_test_95 > 0.5, 1, 0) 
confusion_95 = table(y = R.95$recall, yhat = yhat_test_95) 
confusion_95
table(R.95$recall)
27/sum(table(R.95$recall))
recall.rate= c(.146, .086, .1878, .193, .137)
radiologist= c( "13", "34","66","89","95")
df <- melt(data.frame(radiologist,recall.rate))
keeps <- c("radiologist", "value")
`````

## Part 1
We had the data from five radiologist who saw about 197 patients each. When looking at recall rates for each radiologist we split the patients accordingly. We then created a linear probability model which accounts for the risk factors such as history, symptoms, and age when predicting each recall rate. 

We find that some of these medical professionals are more prone to recalls than others. The middle of our group ended up at a 14.65% (radiologist 13) call back. Radiologist 89 had the highest recall rate at 19.30% and radiologist 34 had the lowest recall rate of 8.63%. This shows there is a range of recall rates dependent on the person seeing the charts. We see our more conservative radiologists, such as 89, and our more lenient radiologist, such as 34. This must be taken into consideration when seeing a radiologist, as the level of conservatism could have a major effect. For reference radiologist 66 had a 18.78% recall rate and 95 had 13.70% recall rate.

 ````{R include=FALSE}
###model A###
logit= glm(cancer ~ recall, data=brca, family='binomial')
phat_test= predict(logit, brca, type='response') 
yhat = ifelse(phat_test> 0.5, 1, 0) 
confusion = table(y = brca$cancer, yhat = yhat) 
confusion
table(brca$cancer)
1/sum(table(brca$recall))
### model B ###
logitB= glm(cancer ~ recall+history+age+symptoms+menopause, data=brca, family='binomial')
coefficients(logitB)
coef(logit) 
phat_testB= predict(logitB, brca, type='response') 
yhatB = ifelse(phat_testB> 0.5, 1, 0) 
````

## Part 2
The purpose of our two models are to see the effects of taking into account factors relating to the patient in whether or not cancer is caught in the first 12 months. 
   
In model A we regressed recall on cancer findings. What we found is that if a patient is recalled they are about 10 times as likely to have cancer. This logic is valid as the more in depth the screening the higher likelihood of cancer being found.
  
In model B we regress with other risk factors to see how they affect the chances of cancer and how the recall rate is effected. What we see is that there is a similar increase in probability of finding the cancer if there is a callback. The remaining risk factors also show an increase in cancer probability when factored in. The exception to this was those patients post-menopause and no hormonal treatment who faced a .787 scale likelihood of cancer being found. Another highlight is that the patients older than 70 are about 3 times more likely for cancer to be discovered. This was the highest risk factor other than being recalled. 

## Conclusion
What we have learned is that when addressing these patients age should be the most heavily weighted in our diagnosis/recall request. The factor of being post-menopause and no hormone therapy should weight the radiologist to not recall the patient for the possibility of having cancer.

# Predicting Viral Articles

## Intro
Here we attempt to determine which articles will go viral and which will not, based on data from Mashable articles.  Then we compared the merits of regressing a numerical number-of-shares variable vs. regressing a binary viral-or-not variable.  Mashable determines the threshold beyond which an article is considered "viral" is 1400 shares.

All predictions were made by training a randomly sampled 80% of the data to make predictions about the covariates, then testing our predictions on the remaining 20%, averaged over 100 different samples.

```{r include=FALSE}
library(foreach)
library(tidyverse)
require(scales)
options(scipen = 999)
online_news <- read.csv("~/Documents/Data Mining/data/online_news.csv")
# Define regressands, "logshares" and "viral"
online_news$logshares = log(online_news$shares)
online_news$viral = ifelse(online_news$shares > 1400, 1, 0)
# Define train/test sizes
n = nrow(online_news)
ntrain = floor(n*0.8)
ntest = n - ntrain
```

## Regress-Then-Threshold
First, we ran a regression on "logshares," and then used that to make predictions.

We found that roughly half of the Mashable articles went viral, but the average number of shares is ~3400 (well beyond the 1400-threshold) which implies that not only do viral articles typically blow past the threshold, but also implies that virality is exponential – an intuitive idea.  A logarithmic transformation to the “shares” variable, then, is a logical way to linearize the relationship between “shares” and the relevant covariates.

Finding which covariates are relevant, however, was messy.  Among the model selection methods we tried were lasso regression, stepwise selection (starting with a basic model), and hand-building models with and without interactions.  The best model turned out to be a hand-built, interaction-free model.  We failed to find any undeniably relevant interactions, and neither the lasso nor stepwise selections seemed to outperform the hand-built model, by R2 or by predictive power, and both cost more in terms of time and intuition, as would a KNN.  (Stepwise returned 59 coefficients!)  In this case, we determined that a simpler model was better and clearer.

```{r include=FALSE, cache=TRUE}
calc_accuracy <- function(formula, splitnum, df) {
  
  fitlist <- list()
  
  for(i in 1:splitnum) {
    
    trainind = sample.int(n, ntrain, replace = FALSE)
    trainset = df[trainind,]
    testset = df[-trainind,]
    
    mash_lm = lm(formula, data = trainset)
    predshares_lm = exp(predict(mash_lm, newdata = testset))
    predviral_lm = ifelse(predshares_lm > 1400, 1, 0)
    
    viral = testset[which(testset$viral==1),]$viral
    notviral = testset[which(testset$viral==0),]$viral
    correctpredviral = ifelse(predviral_lm==1 & testset$viral==1, 1, 0)
    correctprednotviral = ifelse(predviral_lm==0 & testset$viral==0, 1, 0)
    wrongpredviral = ifelse(predviral_lm==1 & testset$viral==0, 1, 0)
    
    fitlist[[i]] <- 
      data.frame(accuracy = (sum(correctprednotviral)+sum(correctpredviral))/7929,
                 nullacc = length(notviral)/7929,
                 error = 1-((sum(correctprednotviral)+sum(correctpredviral))/7929),
               truepos = sum(correctpredviral)/length(viral),
               falsepos = sum(wrongpredviral)/length(notviral)) 
  }
  
  data.table::rbindlist(fitlist, idcol = T)
  
}
linreg = calc_accuracy(logshares ~ n_tokens_title + n_tokens_content + num_self_hrefs + num_hrefs + average_token_length + num_imgs + num_videos + num_keywords + is_weekend + global_rate_positive_words + global_rate_negative_words + title_subjectivity + abs_title_sentiment_polarity + self_reference_avg_sharess + data_channel_is_world + data_channel_is_bus + data_channel_is_entertainment + data_channel_is_tech + data_channel_is_socmed + avg_positive_polarity + avg_negative_polarity,
                       splitnum = 100, df = online_news)
```

### Results
Overall Error Rate:
``` {r echo = FALSE}
mean(linreg$error)
```
True Positive Rate:
``` {r echo = FALSE}
mean(linreg$truepos)
```
False Positive Rate:
``` {r echo = FALSE}
mean(linreg$falsepos)
```
Accuracy of the model that guesses "not viral" for everything on the test set:
``` {r echo = FALSE}
mean(linreg$nullacc)
```
Accuracy of this model:
``` {r echo = FALSE}
mean(linreg$accuracy)
```

## Threshold-Then-Regress
Here we use the same covariates (for a more direct comparison with the regress-first outcomes) with a logit regression model.  For the outcome variable, we defined the binary "viral" as 1 if an article exceeded 1400 shares and 0 if it did not.

``` {r include = FALSE, cache = TRUE}
logit_accuracy <- function(formula, splitnum, df) {
  
  fitlist <- list()
  
  for(i in 1:splitnum) {
    
    trainind = sample.int(n, ntrain, replace = FALSE)
    trainset = df[trainind,]
    testset = df[-trainind,]
    
    mash_logit = glm(formula, data = trainset, family = binomial)
    probviral_logit = predict(mash_logit, newdata = testset, type = "response")
    predviral_logit = ifelse(probviral_logit > 0.5, 1, 0)
    
    viral = testset[which(testset$viral==1),]$viral
    notviral = testset[which(testset$viral==0),]$viral
    correctpredviral = ifelse(predviral_logit==1 & testset$viral==1, 1, 0)
    correctprednotviral = ifelse(predviral_logit==0 & testset$viral==0, 1, 0)
    wrongpredviral = ifelse(predviral_logit==1 & testset$viral==0, 1, 0)
    
    fitlist[[i]] <- 
      data.frame(accuracy = (sum(correctprednotviral)+sum(correctpredviral))/7929,
                 nullacc = length(notviral)/7929,
                 error = 1-((sum(correctprednotviral)+sum(correctpredviral))/7929),
                 truepos = sum(correctpredviral)/length(viral),
                 falsepos = sum(wrongpredviral)/length(notviral)) 
  }
  
  data.table::rbindlist(fitlist, idcol = T)
  
}
logreg = logit_accuracy(viral ~ n_tokens_title + n_tokens_content + num_self_hrefs + num_hrefs + average_token_length + num_imgs + num_videos + num_keywords + is_weekend + global_rate_positive_words + global_rate_negative_words + title_subjectivity + abs_title_sentiment_polarity + self_reference_avg_sharess + data_channel_is_world + data_channel_is_bus + data_channel_is_entertainment + data_channel_is_tech + data_channel_is_socmed + avg_positive_polarity + avg_negative_polarity, splitnum = 100, df = online_news)
```

### Results
Overall Error Rate:
``` {r echo = FALSE}
mean(logreg$error)
```
True Positive Rate:
``` {r echo = FALSE}
mean(logreg$truepos)
```
False Positive Rate:
``` {r echo = FALSE}
mean(logreg$falsepos)
```
Accuracy of the model that guesses "not viral" for everything on the test set:
``` {r echo = FALSE}
mean(logreg$nullacc)
```
Accuracy of this model:
``` {r echo = FALSE}
mean(logreg$accuracy)
```

## Conclusion
We can compare sample confusion matrices to illustrate the difference between the two prediction methods as well.
``` {r include = FALSE}
trainind = sample.int(n, ntrain, replace = FALSE)
trainset = online_news[trainind,]
testset = online_news[-trainind,]
```
### Regress-Then-Threshold Matrix:
``` {r echo = FALSE, cache = TRUE}
mash_lm = lm(logshares ~ n_tokens_title + n_tokens_content + num_self_hrefs + num_hrefs + average_token_length +
               num_imgs + num_videos + num_keywords + is_weekend + global_rate_positive_words + global_rate_negative_words +
               title_subjectivity + abs_title_sentiment_polarity + self_reference_avg_sharess + data_channel_is_world +
               data_channel_is_bus + data_channel_is_entertainment + data_channel_is_tech + data_channel_is_socmed +
               avg_positive_polarity + avg_negative_polarity, data = trainset)
predshares_lm = exp(predict(mash_lm, newdata = testset))
predviral_lm = ifelse(predshares_lm > 1400, 1, 0)
confusion_out_lm = table(viral = testset$viral, viral_pred = predviral_lm)
confusion_out_lm
```
The error rate of this matrix is:
``` {r echo = FALSE}
error_lm = 1-(sum(diag(confusion_out_lm))/sum(confusion_out_lm))
error_lm
```
### Threshold-Then-Regress Matrix:
``` {r echo = FALSE, cache = TRUE}
  # Threshold-then-regress matrix
mash_logit = glm(viral ~ n_tokens_title + n_tokens_content + num_self_hrefs + num_hrefs + average_token_length +
               num_imgs + num_videos + num_keywords + is_weekend + global_rate_positive_words + global_rate_negative_words +
               title_subjectivity + abs_title_sentiment_polarity + self_reference_avg_sharess + data_channel_is_world +
               data_channel_is_bus + data_channel_is_entertainment + data_channel_is_tech + data_channel_is_socmed +
               avg_positive_polarity + avg_negative_polarity, data = trainset, family = binomial)
probviral_logit = predict(mash_logit, newdata = testset, type = "response")
predviral_logit = ifelse(probviral_logit > 0.5, 1, 0)
confusion_out_logit = table(viral = testset$viral, viral_pred = predviral_logit)
confusion_out_logit
```
The error rate of this matrix is:
``` {r echo = FALSE}
error_logit = 1-(sum(diag(confusion_out_logit))/sum(confusion_out_logit))
error_logit
```
The threshold-then-regress approach consistently outperforms the regress-then-threshold approach.  In terms of overall predictive accuracy, the “logshares” regression improves about 8 percentage points and the binary “viral” logit regression about 12 percentage points over the baseline model (i.e. simply choosing the most common result for every result, in this case “not viral”).  Both the true-positive and false-positive rates (predicting virality when it is actually viral and when it is not) are higher for the “logshares” regression, suggesting a higher degree of “optimism” than in the logit model.  We think that since the number of article shares is not inherently linear, predicting the number of shares with a linear regression is tougher than predicting whether it exceeds the viral-threshold.  Transforming the variable logarithmically helps but doesn’t answer for all of a linear model’s bias.  Since the mean number of shares is much higher than the viral-threshold also, the model could bias towards optimistic predictions.  Pegging the virality to a logistic distribution neutralizes the more extreme traits that viral articles carry independently of the covariates (e.g. network effects), so a logit regression is better able to utilize the covariates to make predictions about what we actually want to know, which is simply the likelihood of virality, not the degree of virality.
